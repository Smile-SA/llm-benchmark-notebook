{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "7uhkgSM4vXrv",
        "PNe6F2PImaOF",
        "7vVLOGGamWlZ"
      ],
      "name": "LLMbenchmarkEvals_CelesteDeudon"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš¨ How it works:\n",
        "\n",
        "READ ME:\n",
        "1.  Run the ðŸŒ€ *Setting Up* section, to dowload and install modules needed. Please enter your own Hugging Face token in the token cell (3rd cell in ðŸŒ€ *Setting Up*). Make sure you have requested all accesses as well as signed the agreements needed for the LLMs you are testing.\n",
        "2. In the ðŸŒ€ *TESTING* section, run whichever ðŸ”† *Model* section of the LLM you want to test."
      ],
      "metadata": {
        "id": "byoVUzcSu6uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  ðŸŒ€ Setting Up"
      ],
      "metadata": {
        "id": "xWA5i1bNvR79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers -q\n",
        "! pip install sentencepiece -q\n",
        "! pip install accelerate -q\n",
        "! pip install ipywidgets -q"
      ],
      "metadata": {
        "id": "Ufh51QcXGjU8",
        "execution": {
          "iopub.status.busy": "2024-07-18T13:20:17.140268Z",
          "iopub.execute_input": "2024-07-18T13:20:17.140646Z",
          "iopub.status.idle": "2024-07-18T13:20:56.020702Z",
          "shell.execute_reply.started": "2024-07-18T13:20:17.14061Z",
          "shell.execute_reply": "2024-07-18T13:20:56.019542Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all needed packages\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline #fastest way to use pre-trained models for interferance\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "import re"
      ],
      "metadata": {
        "id": "tivYHlftbXT2",
        "execution": {
          "iopub.status.busy": "2024-07-18T13:21:09.23382Z",
          "iopub.execute_input": "2024-07-18T13:21:09.234136Z",
          "iopub.status.idle": "2024-07-18T13:21:19.692425Z",
          "shell.execute_reply.started": "2024-07-18T13:21:09.234107Z",
          "shell.execute_reply": "2024-07-18T13:21:19.691658Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a token in Hugging Face, and copy it instead of YOUR TOKEN:\n",
        "\n",
        "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('YOUR TOKEN')\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:21:19.694972Z",
          "iopub.execute_input": "2024-07-18T13:21:19.696004Z",
          "iopub.status.idle": "2024-07-18T13:21:21.144546Z",
          "shell.execute_reply.started": "2024-07-18T13:21:19.695963Z",
          "shell.execute_reply": "2024-07-18T13:21:21.14323Z"
        },
        "trusted": true,
        "id": "uFSJ2WOGzamK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ€ TESTING\n",
        "\n",
        "Choose your model within the range of ðŸ”† *Model* section offered (or make a new one and copy the pattern). Running all the cells within a ðŸ”† *Model* section will generate at the end 2 benchmark tables (code + math) for the chosen LLM model.\n",
        "\n",
        "These ðŸ”† *Model* sections each include 11 cells for each LLMs:\n",
        "- Dowloading/Uplaoding the LLM model and tokenizer in question needed for Code Evaluation*\n",
        "- CODE PROMPTING = the actual evaluation of the LLM for code*\n",
        "- the head() to view final result of CODE PROMPTING in a cleaner way\n",
        "- removing the GPU that the LLM model for code occupied using gc (garbage collection)*\n",
        "- checking through smi that the GPU is gone and we have space\n",
        "- Dowloading/Uplaoding the LLM model and tokenizer in question needed for Math Evaluation*\n",
        "- MATH PROMPTING = the actual evaluation of the LLM for math*\n",
        "- the head() to view final result of MATH PROMPTING in a cleaner way\n",
        "- removing the GPU that the LLM model_pipeline for math occupied using gc (garbage collection)*\n",
        "- checking through smi that the GPU is gone and we have space\n",
        "- Formating the Code and Math benchmark tables into a clean viewing experience**\n",
        "\n",
        "Symbol * = Needed, must do\n",
        "\n",
        "Symbol ** = Not mandatory but highly recommended if all needed cells have been executed"
      ],
      "metadata": {
        "id": "7vVLOGGamWlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”† Yi 1.5, by 01.AI (6B)\n",
        "\n",
        "01-ai/Yi-1.5-6B"
      ],
      "metadata": {
        "id": "l3MpkJLLzamL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading/Uplaoding the LLM model and tokenizer in question needed for Code Evaluation\n",
        "\n",
        "#Model bigger than 4B\n",
        "model = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-1.5-6B\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-1.5-6B\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "RORCK5jgrYCD",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE PROMPTING\n",
        "\n",
        "\n",
        "#To prevent this error: Error: cutlassF: no kernel found to launch!\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "\n",
        "# Define the prompts for Code/Python testing\n",
        "prompts = [\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to print 'Hello, World!'. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to count the number of vowels in a given string. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to reverse a given list in-place. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to remove duplicates from a given list. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to check if a given number is even or odd. Print just the function without any further informations.\",\n",
        "]\n",
        "\n",
        "\n",
        "# Define the test names (for columns)\n",
        "\n",
        "test_names = [\n",
        "    \"Print Hello World test\",\n",
        "    \"Vowel Count test\",\n",
        "    \"Reverse List test\",\n",
        "    \"Remove Duplicates test\",\n",
        "    \"Even/Odd test\"\n",
        "]\n",
        "\n",
        "# Create the DataFrame with the test column names\n",
        "benchmarkCodeYi6= pd.DataFrame(index=[\"01-ai/Yi-1.5-6B\"], columns=test_names)\n",
        "\n",
        "\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    try:\n",
        "        # Measure the time it took to generate the code\n",
        "        #torch.manual_seed(0)\n",
        "        start_time = time.time()\n",
        "        # Use the pre-loaded model\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        outputs = model.generate(**inputs, max_length=128)\n",
        "        code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        #print(code) #IF NEED GENERATED CODE PRINTED\n",
        "        end_time = time.time()\n",
        "        generation_time = end_time - start_time\n",
        "\n",
        "        # Extract the function definition from the code\n",
        "        try:\n",
        "            tree = ast.parse(code)\n",
        "            function_def = next((node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)), None)\n",
        "            if function_def:\n",
        "                function_name = function_def.name\n",
        "                code = ast.unparse(function_def)\n",
        "                #print(code) #IF NEED GENERATED WORKING (True) CODE PRINTED\n",
        "            else:\n",
        "                #print(\"invalid\",code) #IF NEED GENERATED NOT WORKING (False) CODE PRINTED\n",
        "                code = None\n",
        "\n",
        "        except SyntaxError:\n",
        "        #    print(e)\n",
        "             code = None\n",
        "\n",
        "        if code:\n",
        "            # Execute the code\n",
        "            exec(code)\n",
        "            # Test the code\n",
        "            if prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to print 'Hello, World!'.\"):\n",
        "                # Test the print helloworld function\n",
        "                result = globals()[function_name]()==  print(\"Hello, World!\")\n",
        "                print(f\"Print Hello World test: {result}\")\n",
        "\n",
        "            elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to count the number of vowels in a given string.\"):\n",
        "                # Test the vowel count function\n",
        "                result = (globals()[function_name](\"hello\") == 2) and (globals()[function_name](\"Maggie Rogers\") == 5)\n",
        "                print(f\"Vowel Count test: {result}\")\n",
        "\n",
        "            elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to reverse a given list in-place.\"):\n",
        "                # Test the reverse list function\n",
        "                list1 = [1, 2, 3, 4, 5]\n",
        "                list2 = [16, 5, 1, 10, 0, 7]\n",
        "                list3 = [3]\n",
        "                list4 = []\n",
        "                result = (globals()[function_name](list1) == [5, 4, 3, 2, 1]) and (globals()[function_name](list2) == [7, 0, 10, 1, 5, 16]) and (globals()[function_name](list3) == [3]) and (globals()[function_name](list4) == [])\n",
        "                print(f\"Reverse List test: {result}\")\n",
        "\n",
        "            elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to remove duplicates from a given list.\"):\n",
        "                # Test the remove duplicates function\n",
        "                result = (globals()[function_name]([1, 1, 2, 2, 3, 3, 4, 5, 5]) == [1, 2, 3, 4, 5]) and (globals()[function_name]([1, 1, 1, 1, 1, 1]) == [1])\n",
        "                print(f\"Remove Duplicates test: {result}\")\n",
        "\n",
        "            elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to check if a given number is even or odd.\"):\n",
        "                # Test the even/odd function\n",
        "                result = (globals()[function_name](10) == True) and (globals()[function_name](11) == False)\n",
        "                print(f\"Even/Odd test: {result}\")\n",
        "\n",
        "            # Store the result in the DataFrame\n",
        "            benchmarkCodeYi6.at[\"01-ai/Yi-1.5-6B\", test_names[i]] = f\"{('Yes' if result else 'No')} ({generation_time:.2f}s)\"\n",
        "    except Exception as e:\n",
        "        # Catch any exceptions, including GPU crashes\n",
        "        benchmarkCodeYi6.at[\"01-ai/Yi-1.5-6B\", test_names[i]] = f\"Error: {e}\"\n",
        "\n",
        "# Print the benchmark table\n",
        "print(benchmarkCodeYi6)"
      ],
      "metadata": {
        "id": "HeTyf5WmhkXL",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmarkCodeYi6.head()"
      ],
      "metadata": {
        "id": "R_GmKMc9h7U6",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "bnif-AdcmR5j",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "XE1mSCkGmSqN",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download and save the model\n",
        "\n",
        "model_pipeline = pipeline(model=\"01-ai/Yi-1.5-6B\",\n",
        "                            torch_dtype=torch.bfloat16, # bytes precisions\n",
        "                            trust_remote_code=True,\n",
        "                            device_map=\"auto\" # will use automatically which best gpu/cpu for model\n",
        "                         )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-1.5-6B\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "Qs7UNnfHxRmM",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_pipeline)"
      ],
      "metadata": {
        "trusted": true,
        "id": "vALPwhUuzamR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "trusted": true,
        "id": "zbyH-Z3mzamS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MATH PROMPTING\n",
        "\n",
        "\n",
        "#To prevent this error: Error: cutlassF: no kernel found to launch!\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "\n",
        "# Define the prompts for Code/Python testing\n",
        "prompts = [\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (3+8)-2 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (6*8)/2 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of 4 to the power of 3 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of squareroot 144 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of the fractions 5/7 + 10/11 ? Print just the result without any further informations.\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# define function that extracts float number after \"is \", so the answer to the prompt\n",
        "\n",
        "def extract_math_answer(text):\n",
        "    # Pattern to match \"is\" followed by a number (including decimals and negative numbers)\n",
        "    pattern = r'is\\s+(-?\\d+(?:\\.\\d+)?)'\n",
        "    # Find the first match in the text\n",
        "    match = re.search(pattern, text)\n",
        "    # If a match is found, return it as a float\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "    # If no match is found, return None\n",
        "    return None\n",
        "\n",
        "\n",
        "# define function prompting the LLM, takes the prompt as parameter\n",
        "\n",
        "def get_completion_model(text):\n",
        "    system = f\"\"\"\n",
        "    You are an expert Mathematician.\n",
        "    You are good at performing and explaining Mathematics concepts in simple words.\n",
        "    Always end your response with 'The answer is X' where X is the final numerical result.\n",
        "    Help as much as you can.\n",
        "    \"\"\"\n",
        "    prompt = f\"#### System: {system}\\n#### User : \\n {text} \\n\\n#### Response from Yi:\"\n",
        "    #print(prompt)\n",
        "    model_response = model_pipeline(prompt, max_new_tokens=500)\n",
        "    answer = model_response[0][\"generated_text\"]\n",
        "    #print(answer)\n",
        "    final = extract_math_answer(answer)\n",
        "    return final\n",
        "\n",
        "\n",
        "# Define the test names (for columns)\n",
        "\n",
        "test_names = [\n",
        "    \"Addition/Subtraction\",\n",
        "    \"Multiplication/Division\",\n",
        "    \"Power\",\n",
        "    \"Square Root\",\n",
        "    \"Fractions\"\n",
        "]\n",
        "\n",
        "# Create the DataFrame with the test column names\n",
        "benchmarkMathYi6 = pd.DataFrame(index=[\"01-ai/Yi-1.5-6B\"], columns=test_names)\n",
        "\n",
        "\n",
        "#Prompt and test to avlaute the LLM\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    try:\n",
        "        # Measure the time it took to generate the code\n",
        "        torch.manual_seed(0)\n",
        "        start_time = time.time()\n",
        "        math = get_completion_model(prompt)\n",
        "        end_time = time.time()\n",
        "        generation_time = end_time - start_time\n",
        "\n",
        "        if math:\n",
        "            # Test the math\n",
        "            if prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (3+8)-2 ?\"):\n",
        "                result = int(math) == 9\n",
        "                print(f\"Add/Sub test: {result}\")\n",
        "            elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (6*8)/2 ?\"):\n",
        "                result = int(math) == 24\n",
        "                print(f\"Mult/Div test: {result}\")\n",
        "            elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of 4 to the power of 3 ?\"):\n",
        "                result = int(math) == 64\n",
        "                print(f\"Power test: {result}\")\n",
        "            elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of squareroot 144 ?\"):\n",
        "                result = int(math) == 12\n",
        "                print(f\"SQRT test: {result}\")\n",
        "            elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of the fractions 5/7 + 10/11 ?\"):\n",
        "                result = math == 125/77\n",
        "                print(f\"Fractions test: {result}\")\n",
        "\n",
        "        # Store the result in the DataFrame using the test name\n",
        "        benchmarkMathYi6.at[\"01-ai/Yi-1.5-6B\", test_names[i]] = f\"{('Yes' if result else 'No')} ({generation_time:.2f}s)\"\n",
        "    except Exception as e:\n",
        "        # Catch any exceptions, including GPU crashes\n",
        "        benchmarkMathYi6.at[\"01-ai/Yi-1.5-6B\", test_names[i]] = f\"Error: {e}\"\n",
        "\n",
        "# Print the benchmark table\n",
        "print(benchmarkMathYi6)"
      ],
      "metadata": {
        "id": "ROw0SngltpN6",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmarkMathYi6.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "RUtLzDNGzamT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "mP64UzGBq4km",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "y9rDm4NJq8YY",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class color:\n",
        "  PURPLE = '\\033[95m'\n",
        "  CYAN = '\\033[96m'\n",
        "  DARKCYAN = '\\033[36m'\n",
        "  BLUE = '\\033[94m'\n",
        "  GREEN = '\\033[92m'\n",
        "  YELLOW = '\\033[93m'\n",
        "  RED = '\\033[91m'\n",
        "  BOLD = '\\033[1m'\n",
        "  UNDERLINE = '\\033[4m'\n",
        "  END = '\\033[0m'\n",
        "\n",
        "\n",
        "\n",
        "# Define the test names for Code\n",
        "test_names_Code = [\n",
        "   \"Print Hello World test\",\n",
        "   \"Vowel Count test\",\n",
        "   \"Reverse List test\",\n",
        "   \"Remove Duplicates test\",\n",
        "   \"Even/Odd test\"\n",
        "]\n",
        "\n",
        "\n",
        "# Define the test names for Math\n",
        "test_names_Math = [\n",
        "   \"Addition/Subtraction\",\n",
        "   \"Multiplication/Division\",\n",
        "   \"Power\",\n",
        "   \"Square Root\",\n",
        "   \"Fractions\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# YI 6B\n",
        "print(color.BOLD + \"\\n \\n EVALUATION for Yi 6B: \\n \\n\" + color.END)\n",
        "\n",
        "#Show the Code Benchmark\n",
        "print(\"Code Evaluations for Yi 6B:\")\n",
        "print(tabulate(benchmarkCodeYi6, headers=test_names_Code, tablefmt='fancy_grid'))\n",
        "\n",
        "#Show the Math Benchmark\n",
        "print(\"\\n \\n Math Evaluations for Yi 6B:\")\n",
        "print(tabulate(benchmarkMathYi6, headers=test_names_Math, tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "id": "f-LLxos6zamU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”† Gemma, by Google (2B)\n",
        "\n",
        "google/gemma-2b"
      ],
      "metadata": {
        "id": "ktUTzsr-zamU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model bigger than 4B\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", trust_remote_code=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tNEWujo1zamU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE PROMPTING\n",
        "\n",
        "\n",
        "#To prevent this error: Error: cutlassF: no kernel found to launch!\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "\n",
        "# Define the prompts for Code/Python testing\n",
        "prompts = [\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to print 'Hello, World!'. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to count the number of vowels in a given string. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to reverse a given list in-place. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to remove duplicates from a given list. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to check if a given number is even or odd. Print just the function without any further informations.\",\n",
        "]\n",
        "\n",
        "\n",
        "# Define the test names (for columns)\n",
        "\n",
        "test_names = [\n",
        "   \"Print Hello World test\",\n",
        "   \"Vowel Count test\",\n",
        "   \"Reverse List test\",\n",
        "   \"Remove Duplicates test\",\n",
        "   \"Even/Odd test\"\n",
        "]\n",
        "\n",
        "\n",
        "# Create the DataFrame with the test column names\n",
        "benchmarkCodeGem2= pd.DataFrame(index=[\"google/gemma-2b\"], columns=test_names)\n",
        "\n",
        "\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "   try:\n",
        "       # Measure the time it took to generate the code\n",
        "       #torch.manual_seed(0)\n",
        "       start_time = time.time()\n",
        "       # Use the pre-loaded model\n",
        "       inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "       outputs = model.generate(**inputs, max_length=128)\n",
        "       code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "       #print(code) #IF NEED GENERATED CODE PRINTED\n",
        "       end_time = time.time()\n",
        "       generation_time = end_time - start_time\n",
        "\n",
        "\n",
        "       # Extract the function definition from the code\n",
        "       try:\n",
        "           tree = ast.parse(code)\n",
        "           function_def = next((node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)), None)\n",
        "           if function_def:\n",
        "               function_name = function_def.name\n",
        "               code = ast.unparse(function_def)\n",
        "               #print(code) #IF NEED GENERATED WORKING (True) CODE PRINTED\n",
        "           else:\n",
        "               #print(\"invalid\",code) #IF NEED GENERATED NOT WORKING (False) CODE PRINTED\n",
        "               code = None\n",
        "\n",
        "\n",
        "       except SyntaxError:\n",
        "       #    print(e)\n",
        "            code = None\n",
        "\n",
        "\n",
        "       if code:\n",
        "           # Execute the code\n",
        "           exec(code)\n",
        "           # Test the code\n",
        "           if prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to print 'Hello, World!'.\"):\n",
        "               # Test the print helloworld function\n",
        "               result = globals()[function_name]()==  print(\"Hello, World!\")\n",
        "               print(f\"Print Hello World test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to count the number of vowels in a given string.\"):\n",
        "               # Test the vowel count function\n",
        "               result = (globals()[function_name](\"hello\") == 2) and (globals()[function_name](\"Maggie Rogers\") == 5)\n",
        "               print(f\"Vowel Count test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to reverse a given list in-place.\"):\n",
        "               # Test the reverse list function\n",
        "               list1 = [1, 2, 3, 4, 5]\n",
        "               list2 = [16, 5, 1, 10, 0, 7]\n",
        "               list3 = [3]\n",
        "               list4 = []\n",
        "               result = (globals()[function_name](list1) == [5, 4, 3, 2, 1]) and (globals()[function_name](list2) == [7, 0, 10, 1, 5, 16]) and (globals()[function_name](list3) == [3]) and (globals()[function_name](list4) == [])\n",
        "               print(f\"Reverse List test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to remove duplicates from a given list.\"):\n",
        "               # Test the remove duplicates function\n",
        "               result = (globals()[function_name]([1, 1, 2, 2, 3, 3, 4, 5, 5]) == [1, 2, 3, 4, 5]) and (globals()[function_name]([1, 1, 1, 1, 1, 1]) == [1])\n",
        "               print(f\"Remove Duplicates test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to check if a given number is even or odd.\"):\n",
        "               # Test the even/odd function\n",
        "               result = (globals()[function_name](10) == True) and (globals()[function_name](11) == False)\n",
        "               print(f\"Even/Odd test: {result}\")\n",
        "\n",
        "\n",
        "           # Store the result in the DataFrame\n",
        "           benchmarkCodeGem2.at[\"google/gemma-2b\", test_names[i]] = f\"{('Yes' if result else 'No')} ({generation_time:.2f}s)\"\n",
        "   except Exception as e:\n",
        "       # Catch any exceptions, including GPU crashes\n",
        "       benchmarkCodeGem2.at[\"google/gemma-2b\", test_names[i]] = f\"Error: {e}\"\n",
        "\n",
        "\n",
        "# Print the benchmark table\n",
        "print(benchmarkCodeGem2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "kuzgILQtzamV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmarkCodeGem2.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "uhZMftZ2zamV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "id": "aadwrArPzamW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "trusted": true,
        "id": "jcNS9gp5zamW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download and save the model\n",
        "\n",
        "model_pipeline = pipeline(model=\"google/gemma-2b\",\n",
        "                           torch_dtype=torch.bfloat16, # bytes precisions\n",
        "                           trust_remote_code=True,\n",
        "                           device_map=\"auto\" # will use automatically which best gpu/cpu for model\n",
        "                        )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", trust_remote_code=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bwPuJQM_zamX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MATH PROMPTING\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "import re\n",
        "\n",
        "\n",
        "#To prevent this error: Error: cutlassF: no kernel found to launch!\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "\n",
        "# Define the prompts for Code/Python testing\n",
        "prompts = [\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (3+8)-2 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (6*8)/2 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of 4 to the power of 3 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of squareroot 144 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of the fractions 5/7 + 10/11 ? Print just the result without any further informations.\"\n",
        "]\n",
        "\n",
        "\n",
        "# define function that extracts float number after \"is \", so the answer to the prompt\n",
        "\n",
        "def extract_math_answer(text):\n",
        "   # Pattern to match \"is\" followed by a number (including decimals and negative numbers)\n",
        "   pattern = r'is\\s+(-?\\d+(?:\\.\\d+)?)'\n",
        "   # Find the first match in the text\n",
        "   match = re.search(pattern, text)\n",
        "   # If a match is found, return it as a float\n",
        "   if match:\n",
        "       return float(match.group(1))\n",
        "   # If no match is found, return None\n",
        "   return None\n",
        "\n",
        "\n",
        "# define function prompting the LLM, takes the prompt as parameter\n",
        "\n",
        "def get_completion_model(text):\n",
        "   system = f\"\"\"\n",
        "   You are an expert Mathematician.\n",
        "   You are good at performing and explaining Mathematics concepts in simple words.\n",
        "   Always end your response with 'The answer is X' where X is the final numerical result.\n",
        "   Help as much as you can.\n",
        "   \"\"\"\n",
        "   prompt = f\"#### System: {system}\\n#### User : \\n {text} \\n\\n#### Response from Gemma:\"\n",
        "   #print(prompt)\n",
        "   model_response = model_pipeline(prompt, max_new_tokens=500)\n",
        "   answer = model_response[0][\"generated_text\"]\n",
        "   #print(answer)\n",
        "   final = extract_math_answer(answer)\n",
        "   return final\n",
        "\n",
        "\n",
        "\n",
        "# Define the test names (for columns)\n",
        "\n",
        "test_names = [\n",
        "   \"Addition/Subtraction\",\n",
        "   \"Multiplication/Division\",\n",
        "   \"Power\",\n",
        "   \"Square Root\",\n",
        "   \"Fractions\"\n",
        "]\n",
        "\n",
        "\n",
        "# Create the DataFrame with the test column names\n",
        "benchmarkMathGem2 = pd.DataFrame(index=[\"google/gemma-2b\"], columns=test_names)\n",
        "\n",
        "\n",
        "#Prompt and test to evaluate the LLM\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "   try:\n",
        "       # Measure the time it took to generate the code\n",
        "       torch.manual_seed(0)\n",
        "       start_time = time.time()\n",
        "       math = get_completion_model(prompt)\n",
        "       end_time = time.time()\n",
        "       generation_time = end_time - start_time\n",
        "\n",
        "       if math:\n",
        "           # Test the math\n",
        "           if prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (3+8)-2 ?\"):\n",
        "               result = int(math) == 9\n",
        "               print(f\"Add/Sub test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (6*8)/2 ?\"):\n",
        "               result = int(math) == 24\n",
        "               print(f\"Mult/Div test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of 4 to the power of 3 ?\"):\n",
        "               result = int(math) == 64\n",
        "               print(f\"Power test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of squareroot 144 ?\"):\n",
        "               result = int(math) == 12\n",
        "               print(f\"SQRT test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of the fractions 5/7 + 10/11 ?\"):\n",
        "               result = math == 125/77\n",
        "               print(f\"Fractions test: {result}\")\n",
        "\n",
        "       # Store the result in the DataFrame using the test name\n",
        "       benchmarkMathGem2.at[\"google/gemma-2b\", test_names[i]] = f\"{('Yes' if result else 'No')} ({generation_time:.2f}s)\"\n",
        "   except Exception as e:\n",
        "       # Catch any exceptions, including GPU crashes\n",
        "       benchmarkMathGem2.at[\"google/gemma-2b\", test_names[i]] = f\"Error: {e}\"\n",
        "\n",
        "\n",
        "# Print the benchmark table\n",
        "print(benchmarkMathGem2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "LOH5W7OBzamX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmarkMathGem2.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "FpaBj53nzamX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "id": "BYkFyrePzamY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q6X925srzamY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "class color:\n",
        "  PURPLE = '\\033[95m'\n",
        "  CYAN = '\\033[96m'\n",
        "  DARKCYAN = '\\033[36m'\n",
        "  BLUE = '\\033[94m'\n",
        "  GREEN = '\\033[92m'\n",
        "  YELLOW = '\\033[93m'\n",
        "  RED = '\\033[91m'\n",
        "  BOLD = '\\033[1m'\n",
        "  UNDERLINE = '\\033[4m'\n",
        "  END = '\\033[0m'\n",
        "\n",
        "\n",
        "\n",
        "# Define the test names for Code\n",
        "test_names_Code = [\n",
        "   \"Print Hello World test\",\n",
        "   \"Vowel Count test\",\n",
        "   \"Reverse List test\",\n",
        "   \"Remove Duplicates test\",\n",
        "   \"Even/Odd test\"\n",
        "]\n",
        "\n",
        "\n",
        "# Define the test names for Math\n",
        "test_names_Math = [\n",
        "   \"Addition/Subtraction\",\n",
        "   \"Multiplication/Division\",\n",
        "   \"Power\",\n",
        "   \"Square Root\",\n",
        "   \"Fractions\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Gemma 2B\n",
        "print(color.BOLD + \"\\n \\n EVALUATION for Gemma 2B: \\n \\n\" + color.END)\n",
        "\n",
        "#Show the Code Benchmark\n",
        "print(\"Code Evaluations for Gemma 2B:\")\n",
        "print(tabulate(benchmarkCodeGem2, headers=test_names_Code, tablefmt='fancy_grid'))\n",
        "\n",
        "#Show the Math Benchmark\n",
        "print(\"\\n \\n Math Evaluations for Gemma 2B:\")\n",
        "print(tabulate(benchmarkMathGem2, headers=test_names_Math, tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "trusted": true,
        "id": "162yTn2NzamY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”† Vicuna, by LMSYS Org (7B)\n",
        "\n",
        "lmsys/vicuna-7b-v1.3"
      ],
      "metadata": {
        "id": "Jxpc_2zozamY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model bigger than 4B\n",
        "model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.3\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.3\", trust_remote_code=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NHXWH-bNzamZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE PROMPTING\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "\n",
        "\n",
        "#To prevent this error: Error: cutlassF: no kernel found to launch!\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompts for Code/Python testing\n",
        "prompts = [\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to print 'Hello, World!'. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to count the number of vowels in a given string. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to reverse a given list in-place. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to remove duplicates from a given list. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to check if a given number is even or odd. Print just the function without any further informations.\",\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the test names (for columns)\n",
        "\n",
        "\n",
        "test_names = [\n",
        "   \"Print Hello World test\",\n",
        "   \"Vowel Count test\",\n",
        "   \"Reverse List test\",\n",
        "   \"Remove Duplicates test\",\n",
        "   \"Even/Odd test\"\n",
        "]\n",
        "\n",
        "\n",
        "# Create the DataFrame with the test column names\n",
        "benchmarkCodeVi7= pd.DataFrame(index=[\"lmsys/vicuna-7b-v1.3\"], columns=test_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "   try:\n",
        "       # Measure the time it took to generate the code\n",
        "       #torch.manual_seed(0)\n",
        "       start_time = time.time()\n",
        "       # Use the pre-loaded model\n",
        "       inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "       outputs = model.generate(**inputs, max_length=128)\n",
        "       code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "       #print(code) #IF NEED GENERATED CODE PRINTED\n",
        "       end_time = time.time()\n",
        "       generation_time = end_time - start_time\n",
        "\n",
        "\n",
        "       # Extract the function definition from the code\n",
        "       try:\n",
        "           tree = ast.parse(code)\n",
        "           function_def = next((node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)), None)\n",
        "           if function_def:\n",
        "               function_name = function_def.name\n",
        "               code = ast.unparse(function_def)\n",
        "               #print(code) #IF NEED GENERATED WORKING (True) CODE PRINTED\n",
        "           else:\n",
        "               #print(\"invalid\",code) #IF NEED GENERATED NOT WORKING (False) CODE PRINTED\n",
        "               code = None\n",
        "\n",
        "\n",
        "       except SyntaxError:\n",
        "       #    print(e)\n",
        "            code = None\n",
        "\n",
        "\n",
        "       if code:\n",
        "           # Execute the code\n",
        "           exec(code)\n",
        "           # Test the code\n",
        "           if prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to print 'Hello, World!'.\"):\n",
        "               # Test the print helloworld function\n",
        "               result = globals()[function_name]()==  print(\"Hello, World!\")\n",
        "               print(f\"Print Hello World test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to count the number of vowels in a given string.\"):\n",
        "               # Test the vowel count function\n",
        "               result = (globals()[function_name](\"hello\") == 2) and (globals()[function_name](\"Maggie Rogers\") == 5)\n",
        "               print(f\"Vowel Count test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to reverse a given list in-place.\"):\n",
        "               # Test the reverse list function\n",
        "               list1 = [1, 2, 3, 4, 5]\n",
        "               list2 = [16, 5, 1, 10, 0, 7]\n",
        "               list3 = [3]\n",
        "               list4 = []\n",
        "               result = (globals()[function_name](list1) == [5, 4, 3, 2, 1]) and (globals()[function_name](list2) == [7, 0, 10, 1, 5, 16]) and (globals()[function_name](list3) == [3]) and (globals()[function_name](list4) == [])\n",
        "               print(f\"Reverse List test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to remove duplicates from a given list.\"):\n",
        "               # Test the remove duplicates function\n",
        "               result = (globals()[function_name]([1, 1, 2, 2, 3, 3, 4, 5, 5]) == [1, 2, 3, 4, 5]) and (globals()[function_name]([1, 1, 1, 1, 1, 1]) == [1])\n",
        "               print(f\"Remove Duplicates test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to check if a given number is even or odd.\"):\n",
        "               # Test the even/odd function\n",
        "               result = (globals()[function_name](10) == True) and (globals()[function_name](11) == False)\n",
        "               print(f\"Even/Odd test: {result}\")\n",
        "\n",
        "\n",
        "           # Store the result in the DataFrame\n",
        "           benchmarkCodeVi7.at[\"lmsys/vicuna-7b-v1.3\", test_names[i]] = f\"{('Yes' if result else 'No')} ({generation_time:.2f}s)\"\n",
        "   except Exception as e:\n",
        "       # Catch any exceptions, including GPU crashes\n",
        "       benchmarkCodeVi7.at[\"lmsys/vicuna-7b-v1.3\", test_names[i]] = f\"Error: {e}\"\n",
        "\n",
        "\n",
        "# Print the benchmark table\n",
        "print(benchmarkCodeVi7)"
      ],
      "metadata": {
        "trusted": true,
        "id": "35aqAqBwzamZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmarkCodeVi7.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "vOPXM_Hmzama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "id": "V6dd-f2Jzama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "trusted": true,
        "id": "pEctPScYzama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download and save the model\n",
        "\n",
        "\n",
        "model_pipeline = pipeline(model=\"lmsys/vicuna-7b-v1.3\",\n",
        "                           torch_dtype=torch.bfloat16, # bytes precisions\n",
        "                           trust_remote_code=True,\n",
        "                           device_map=\"auto\" # will use automatically which best gpu/cpu for model\n",
        "                        )\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.3\", trust_remote_code=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "gR_Y7_44zama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MATH PROMPTING\n",
        "\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#To prevent this error: Error: cutlassF: no kernel found to launch!\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompts for Code/Python testing\n",
        "prompts = [\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (3+8)-2 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (6*8)/2 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of 4 to the power of 3 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of squareroot 144 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of the fractions 5/7 + 10/11 ? Print just the result without any further informations.\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define function that extracts float number after \"is \", so the answer to the prompt\n",
        "\n",
        "\n",
        "def extract_math_answer(text):\n",
        "   # Pattern to match \"is\" followed by a number (including decimals and negative numbers)\n",
        "   pattern = r'is\\s+(-?\\d+(?:\\.\\d+)?)'\n",
        "   # Find the first match in the text\n",
        "   match = re.search(pattern, text)\n",
        "   # If a match is found, return it as a float\n",
        "   if match:\n",
        "       return float(match.group(1))\n",
        "   # If no match is found, return None\n",
        "   return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define function prompting the LLM, takes the prompt as parameter\n",
        "\n",
        "\n",
        "def get_completion_model(text):\n",
        "   system = f\"\"\"\n",
        "   You are an expert Mathematician.\n",
        "   You are good at performing and explaining Mathematics concepts in simple words.\n",
        "   Always end your response with 'The answer is X' where X is the final numerical result.\n",
        "   Help as much as you can.\n",
        "   \"\"\"\n",
        "   prompt = f\"#### System: {system}\\n#### User : \\n {text} \\n\\n#### Response from Yi:\"\n",
        "   #print(prompt)\n",
        "   model_response = model_pipeline(prompt, max_new_tokens=500)\n",
        "   answer = model_response[0][\"generated_text\"]\n",
        "   #print(answer)\n",
        "   final = extract_math_answer(answer)\n",
        "   return final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the test names (for columns)\n",
        "\n",
        "\n",
        "test_names = [\n",
        "   \"Addition/Subtraction\",\n",
        "   \"Multiplication/Division\",\n",
        "   \"Power\",\n",
        "   \"Square Root\",\n",
        "   \"Fractions\"\n",
        "]\n",
        "\n",
        "\n",
        "# Create the DataFrame with the test column names\n",
        "benchmarkMathVi7 = pd.DataFrame(index=[\"lmsys/vicuna-7b-v1.3\"], columns=test_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Prompt and test to evaluate the LLM\n",
        "\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "   try:\n",
        "       # Measure the time it took to generate the code\n",
        "       torch.manual_seed(0)\n",
        "       start_time = time.time()\n",
        "       math = get_completion_model(prompt)\n",
        "       end_time = time.time()\n",
        "       generation_time = end_time - start_time\n",
        "\n",
        "       if math:\n",
        "           # Test the math\n",
        "           if prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (3+8)-2 ?\"):\n",
        "               result = int(math) == 9\n",
        "               print(f\"Add/Sub test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (6*8)/2 ?\"):\n",
        "               result = int(math) == 24\n",
        "               print(f\"Mult/Div test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of 4 to the power of 3 ?\"):\n",
        "               result = int(math) == 64\n",
        "               print(f\"Power test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of squareroot 144 ?\"):\n",
        "               result = int(math) == 12\n",
        "               print(f\"SQRT test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of the fractions 5/7 + 10/11 ?\"):\n",
        "               result = math == 125/77\n",
        "               print(f\"Fractions test: {result}\")\n",
        "\n",
        "       # Store the result in the DataFrame using the test name\n",
        "       benchmarkMathVi7.at[\"lmsys/vicuna-7b-v1.3\", test_names[i]] = f\"{('Yes' if result else 'No')} ({generation_time:.2f}s)\"\n",
        "   except Exception as e:\n",
        "       # Catch any exceptions, including GPU crashes\n",
        "       benchmarkMathVi7.at[\"lmsys/vicuna-7b-v1.3\", test_names[i]] = f\"Error: {e}\"\n",
        "\n",
        "\n",
        "# Print the benchmark table\n",
        "print(benchmarkMathVi7)"
      ],
      "metadata": {
        "trusted": true,
        "id": "uccINkuqzama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmarkMathVi7.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "JlFN_m0jzama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "id": "oWW-nSPFzamb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "trusted": true,
        "id": "bUhaXcy6zamb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "class color:\n",
        "  PURPLE = '\\033[95m'\n",
        "  CYAN = '\\033[96m'\n",
        "  DARKCYAN = '\\033[36m'\n",
        "  BLUE = '\\033[94m'\n",
        "  GREEN = '\\033[92m'\n",
        "  YELLOW = '\\033[93m'\n",
        "  RED = '\\033[91m'\n",
        "  BOLD = '\\033[1m'\n",
        "  UNDERLINE = '\\033[4m'\n",
        "  END = '\\033[0m'\n",
        "\n",
        "\n",
        "\n",
        "# Define the test names for Code\n",
        "test_names_Code = [\n",
        "   \"Print Hello World test\",\n",
        "   \"Vowel Count test\",\n",
        "   \"Reverse List test\",\n",
        "   \"Remove Duplicates test\",\n",
        "   \"Even/Odd test\"\n",
        "]\n",
        "\n",
        "\n",
        "# Define the test names for Math\n",
        "test_names_Math = [\n",
        "   \"Addition/Subtraction\",\n",
        "   \"Multiplication/Division\",\n",
        "   \"Power\",\n",
        "   \"Square Root\",\n",
        "   \"Fractions\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Vicuna 7B\n",
        "print(color.BOLD + \"\\n \\n EVALUATION for Vicuna 7B: \\n \\n\" + color.END)\n",
        "\n",
        "\n",
        "#Show the Code Benchmark\n",
        "print(\"Code Evaluations for Vicuna 7B:\")\n",
        "print(tabulate(benchmarkCodeVi7, headers=test_names_Code, tablefmt='fancy_grid'))\n",
        "\n",
        "\n",
        "#Show the Math Benchmark\n",
        "print(\"\\n \\n Math Evaluations for Vicuna 7B:\")\n",
        "print(tabulate(benchmarkMathVi7, headers=test_names_Math, tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "trusted": true,
        "id": "xJegMENgzamb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”† Mistral, by Mistral AI (7B)\n",
        "\n",
        "\n",
        "mistralai/Mistral-7B-v0.1"
      ],
      "metadata": {
        "id": "YCiLEf0Ozamc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model bigger than 4B\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", trust_remote_code=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:21:21.146178Z",
          "iopub.execute_input": "2024-07-18T13:21:21.14656Z",
          "iopub.status.idle": "2024-07-18T13:23:27.628741Z",
          "shell.execute_reply.started": "2024-07-18T13:21:21.146527Z",
          "shell.execute_reply": "2024-07-18T13:23:27.627853Z"
        },
        "trusted": true,
        "id": "AcHHuMIczamc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE PROMPTING\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "\n",
        "\n",
        "#To prevent this error: Error: cutlassF: no kernel found to launch!\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompts for Code/Python testing\n",
        "prompts = [\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to print 'Hello, World!'. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to count the number of vowels in a given string. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to reverse a given list in-place. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to remove duplicates from a given list. Print just the function without any further informations.\",\n",
        "\"#You are an expert Python programmer, and here is your task: Write a Python function to check if a given number is even or odd. Print just the function without any further informations.\",\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the test names (for columns)\n",
        "\n",
        "\n",
        "test_names = [\n",
        "   \"Print Hello World test\",\n",
        "   \"Vowel Count test\",\n",
        "   \"Reverse List test\",\n",
        "   \"Remove Duplicates test\",\n",
        "   \"Even/Odd test\"\n",
        "]\n",
        "\n",
        "\n",
        "# Create the DataFrame with the test column names\n",
        "benchmarkCodeMi7= pd.DataFrame(index=[\"mistralai/Mistral-7B-v0.1\"], columns=test_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "   try:\n",
        "       # Measure the time it took to generate the code\n",
        "       #torch.manual_seed(0)\n",
        "       start_time = time.time()\n",
        "       # Use the pre-loaded model\n",
        "       inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "       outputs = model.generate(**inputs, max_length=128)\n",
        "       code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "       #print(code) #IF NEED GENERATED CODE PRINTED\n",
        "       end_time = time.time()\n",
        "       generation_time = end_time - start_time\n",
        "\n",
        "\n",
        "       # Extract the function definition from the code\n",
        "       try:\n",
        "           tree = ast.parse(code)\n",
        "           function_def = next((node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)), None)\n",
        "           if function_def:\n",
        "               function_name = function_def.name\n",
        "               code = ast.unparse(function_def)\n",
        "               #print(code) #IF NEED GENERATED WORKING (True) CODE PRINTED\n",
        "           else:\n",
        "               #print(\"invalid\",code) #IF NEED GENERATED NOT WORKING (False) CODE PRINTED\n",
        "               code = None\n",
        "\n",
        "\n",
        "       except SyntaxError:\n",
        "       #    print(e)\n",
        "            code = None\n",
        "\n",
        "\n",
        "       if code:\n",
        "           # Execute the code\n",
        "           exec(code)\n",
        "           # Test the code\n",
        "           if prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to print 'Hello, World!'.\"):\n",
        "               # Test the print helloworld function\n",
        "               result = globals()[function_name]()==  print(\"Hello, World!\")\n",
        "               print(f\"Print Hello World test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to count the number of vowels in a given string.\"):\n",
        "               # Test the vowel count function\n",
        "               result = (globals()[function_name](\"hello\") == 2) and (globals()[function_name](\"Maggie Rogers\") == 5)\n",
        "               print(f\"Vowel Count test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to reverse a given list in-place.\"):\n",
        "               # Test the reverse list function\n",
        "               list1 = [1, 2, 3, 4, 5]\n",
        "               list2 = [16, 5, 1, 10, 0, 7]\n",
        "               list3 = [3]\n",
        "               list4 = []\n",
        "               result = (globals()[function_name](list1) == [5, 4, 3, 2, 1]) and (globals()[function_name](list2) == [7, 0, 10, 1, 5, 16]) and (globals()[function_name](list3) == [3]) and (globals()[function_name](list4) == [])\n",
        "               print(f\"Reverse List test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to remove duplicates from a given list.\"):\n",
        "               # Test the remove duplicates function\n",
        "               result = (globals()[function_name]([1, 1, 2, 2, 3, 3, 4, 5, 5]) == [1, 2, 3, 4, 5]) and (globals()[function_name]([1, 1, 1, 1, 1, 1]) == [1])\n",
        "               print(f\"Remove Duplicates test: {result}\")\n",
        "\n",
        "\n",
        "           elif prompt.startswith(\"#You are an expert Python programmer, and here is your task: Write a Python function to check if a given number is even or odd.\"):\n",
        "               # Test the even/odd function\n",
        "               result = (globals()[function_name](10) == True) and (globals()[function_name](11) == False)\n",
        "               print(f\"Even/Odd test: {result}\")\n",
        "\n",
        "\n",
        "           # Store the result in the DataFrame\n",
        "           benchmarkCodeMi7.at[\"mistralai/Mistral-7B-v0.1\", test_names[i]] = f\"{('Yes' if result else 'No')} ({generation_time:.2f}s)\"\n",
        "   except Exception as e:\n",
        "       # Catch any exceptions, including GPU crashes\n",
        "       benchmarkCodeMi7.at[\"mistralai/Mistral-7B-v0.1\", test_names[i]] = f\"Error: {e}\"\n",
        "\n",
        "\n",
        "# Print the benchmark table\n",
        "print(benchmarkCodeMi7)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:24:43.801262Z",
          "iopub.execute_input": "2024-07-18T13:24:43.802095Z",
          "iopub.status.idle": "2024-07-18T13:25:05.51895Z",
          "shell.execute_reply.started": "2024-07-18T13:24:43.80206Z",
          "shell.execute_reply": "2024-07-18T13:25:05.51785Z"
        },
        "trusted": true,
        "id": "RlaPrX8Hzamd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmarkCodeMi7.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:25:24.491157Z",
          "iopub.execute_input": "2024-07-18T13:25:24.491597Z",
          "iopub.status.idle": "2024-07-18T13:25:24.506103Z",
          "shell.execute_reply.started": "2024-07-18T13:25:24.491559Z",
          "shell.execute_reply": "2024-07-18T13:25:24.505065Z"
        },
        "trusted": true,
        "id": "PJUxGsl5zamd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:25:36.499843Z",
          "iopub.execute_input": "2024-07-18T13:25:36.500531Z",
          "iopub.status.idle": "2024-07-18T13:25:37.309985Z",
          "shell.execute_reply.started": "2024-07-18T13:25:36.500497Z",
          "shell.execute_reply": "2024-07-18T13:25:37.308911Z"
        },
        "trusted": true,
        "id": "oLpvXVkvzamd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:25:44.35879Z",
          "iopub.execute_input": "2024-07-18T13:25:44.359164Z",
          "iopub.status.idle": "2024-07-18T13:25:45.475087Z",
          "shell.execute_reply.started": "2024-07-18T13:25:44.359134Z",
          "shell.execute_reply": "2024-07-18T13:25:45.473905Z"
        },
        "trusted": true,
        "id": "PEu-tZpRzame"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download and save the model\n",
        "\n",
        "\n",
        "model_pipeline = pipeline(model=\"mistralai/Mistral-7B-v0.1\",\n",
        "                           torch_dtype=torch.bfloat16, # bytes precisions\n",
        "                           trust_remote_code=True,\n",
        "                           device_map=\"auto\" # will use automatically which best gpu/cpu for model\n",
        "                        )\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", trust_remote_code=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:26:31.966109Z",
          "iopub.execute_input": "2024-07-18T13:26:31.966975Z",
          "iopub.status.idle": "2024-07-18T13:27:03.54247Z",
          "shell.execute_reply.started": "2024-07-18T13:26:31.966937Z",
          "shell.execute_reply": "2024-07-18T13:27:03.541637Z"
        },
        "trusted": true,
        "id": "yLkUYT_Hzame"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MATH PROMPTING\n",
        "\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#To prevent this error: Error: cutlassF: no kernel found to launch!\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the prompts for Code/Python testing\n",
        "prompts = [\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (3+8)-2 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (6*8)/2 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of 4 to the power of 3 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of squareroot 144 ? Print just the result without any further informations.\",\n",
        "\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of the fractions 5/7 + 10/11 ? Print just the result without any further informations.\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define function that extracts float number after \"is \", so the answer to the prompt\n",
        "\n",
        "\n",
        "def extract_math_answer(text):\n",
        "   # Pattern to match \"is\" followed by a number (including decimals and negative numbers)\n",
        "   pattern = r'is\\s+(-?\\d+(?:\\.\\d+)?)'\n",
        "   # Find the first match in the text\n",
        "   match = re.search(pattern, text)\n",
        "   # If a match is found, return it as a float\n",
        "   if match:\n",
        "       return float(match.group(1))\n",
        "   # If no match is found, return None\n",
        "   return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define function prompting the LLM, takes the prompt as parameter\n",
        "\n",
        "\n",
        "def get_completion_model(text):\n",
        "   system = f\"\"\"\n",
        "   You are an expert Mathematician.\n",
        "   You are good at performing and explaining Mathematics concepts in simple words.\n",
        "   Always end your response with 'The answer is X' where X is the final numerical result.\n",
        "   Help as much as you can.\n",
        "   \"\"\"\n",
        "   prompt = f\"#### System: {system}\\n#### User : \\n {text} \\n\\n#### Response from Yi:\"\n",
        "   #print(prompt)\n",
        "   model_response = model_pipeline(prompt, max_new_tokens=500)\n",
        "   answer = model_response[0][\"generated_text\"]\n",
        "   #print(answer)\n",
        "   final = extract_math_answer(answer)\n",
        "   return final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the test names (for columns)\n",
        "\n",
        "\n",
        "test_names = [\n",
        "   \"Addition/Subtraction\",\n",
        "   \"Multiplication/Division\",\n",
        "   \"Power\",\n",
        "   \"Square Root\",\n",
        "   \"Fractions\"\n",
        "]\n",
        "\n",
        "\n",
        "# Create the DataFrame with the test column names\n",
        "benchmarkMathMi7 = pd.DataFrame(index=[\"mistralai/Mistral-7B-v0.1\"], columns=test_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Prompt and test to evaluate the LLM\n",
        "\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "   try:\n",
        "       # Measure the time it took to generate the code\n",
        "       torch.manual_seed(0)\n",
        "       start_time = time.time()\n",
        "       math = get_completion_model(prompt)\n",
        "       end_time = time.time()\n",
        "       generation_time = end_time - start_time\n",
        "\n",
        "       if math:\n",
        "           # Test the math\n",
        "           if prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (3+8)-2 ?\"):\n",
        "               result = int(math) == 9\n",
        "               print(f\"Add/Sub test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of (6*8)/2 ?\"):\n",
        "               result = int(math) == 24\n",
        "               print(f\"Mult/Div test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of 4 to the power of 3 ?\"):\n",
        "               result = int(math) == 64\n",
        "               print(f\"Power test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of squareroot 144 ?\"):\n",
        "               result = int(math) == 12\n",
        "               print(f\"SQRT test: {result}\")\n",
        "           elif prompt.startswith(\"You are an expert Mathematician. You are good at performing and explaining Mathematics concepts in simple words. Help as much as you can. Here is your problem: What is the solution of the fractions 5/7 + 10/11 ?\"):\n",
        "               result = math == 125/77\n",
        "               print(f\"Fractions test: {result}\")\n",
        "\n",
        "       # Store the result in the DataFrame using the test name\n",
        "       benchmarkMathMi7.at[\"mistralai/Mistral-7B-v0.1\", test_names[i]] = f\"{('Yes' if result else 'No')} ({generation_time:.2f}s)\"\n",
        "   except Exception as e:\n",
        "       # Catch any exceptions, including GPU crashes\n",
        "       benchmarkMathMi7.at[\"mistralai/Mistral-7B-v0.1\", test_names[i]] = f\"Error: {e}\"\n",
        "\n",
        "\n",
        "# Print the benchmark table\n",
        "print(benchmarkMathMi7)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:27:35.513252Z",
          "iopub.execute_input": "2024-07-18T13:27:35.513748Z",
          "iopub.status.idle": "2024-07-18T13:29:44.710604Z",
          "shell.execute_reply.started": "2024-07-18T13:27:35.513714Z",
          "shell.execute_reply": "2024-07-18T13:29:44.709544Z"
        },
        "trusted": true,
        "id": "bXRqGpHTzamf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmarkMathMi7.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:30:38.123699Z",
          "iopub.execute_input": "2024-07-18T13:30:38.124491Z",
          "iopub.status.idle": "2024-07-18T13:30:38.135441Z",
          "shell.execute_reply.started": "2024-07-18T13:30:38.124454Z",
          "shell.execute_reply": "2024-07-18T13:30:38.134428Z"
        },
        "trusted": true,
        "id": "0oa4EiUAzamg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:30:39.992958Z",
          "iopub.execute_input": "2024-07-18T13:30:39.993573Z",
          "iopub.status.idle": "2024-07-18T13:30:40.808832Z",
          "shell.execute_reply.started": "2024-07-18T13:30:39.993541Z",
          "shell.execute_reply": "2024-07-18T13:30:40.807786Z"
        },
        "trusted": true,
        "id": "BvWRUPNnzamh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:30:41.771689Z",
          "iopub.execute_input": "2024-07-18T13:30:41.772075Z",
          "iopub.status.idle": "2024-07-18T13:30:42.850397Z",
          "shell.execute_reply.started": "2024-07-18T13:30:41.772041Z",
          "shell.execute_reply": "2024-07-18T13:30:42.849377Z"
        },
        "trusted": true,
        "id": "VTMF7lf0zami"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "class color:\n",
        "  PURPLE = '\\033[95m'\n",
        "  CYAN = '\\033[96m'\n",
        "  DARKCYAN = '\\033[36m'\n",
        "  BLUE = '\\033[94m'\n",
        "  GREEN = '\\033[92m'\n",
        "  YELLOW = '\\033[93m'\n",
        "  RED = '\\033[91m'\n",
        "  BOLD = '\\033[1m'\n",
        "  UNDERLINE = '\\033[4m'\n",
        "  END = '\\033[0m'\n",
        "\n",
        "\n",
        "\n",
        "# Define the test names for Code\n",
        "test_names_Code = [\n",
        "   \"Print Hello World test\",\n",
        "   \"Vowel Count test\",\n",
        "   \"Reverse List test\",\n",
        "   \"Remove Duplicates test\",\n",
        "   \"Even/Odd test\"\n",
        "]\n",
        "\n",
        "\n",
        "# Define the test names for Math\n",
        "test_names_Math = [\n",
        "   \"Addition/Subtraction\",\n",
        "   \"Multiplication/Division\",\n",
        "   \"Power\",\n",
        "   \"Square Root\",\n",
        "   \"Fractions\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Mistral 7B\n",
        "print(color.BOLD + \"\\n \\n EVALUATION for Mistral 7B: \\n \\n\" + color.END)\n",
        "\n",
        "\n",
        "#Show the Code Benchmark\n",
        "print(\"Code Evaluations for Mistral 7B:\")\n",
        "print(tabulate(benchmarkCodeMi7, headers=test_names_Code, tablefmt='fancy_grid'))\n",
        "\n",
        "\n",
        "#Show the Math Benchmark\n",
        "print(\"\\n \\n Math Evaluations for Mistral 7B:\")\n",
        "print(tabulate(benchmarkMathMi7, headers=test_names_Math, tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-18T13:30:43.824102Z",
          "iopub.execute_input": "2024-07-18T13:30:43.824531Z",
          "iopub.status.idle": "2024-07-18T13:30:43.848292Z",
          "shell.execute_reply.started": "2024-07-18T13:30:43.824492Z",
          "shell.execute_reply": "2024-07-18T13:30:43.847212Z"
        },
        "trusted": true,
        "id": "xJNBWfFQzami"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}