README: 

1. Project Title: Evaluation of LLMs, benchmarking in code (python) and math. 



2. Project Description: 
Notebook format for evaluating solutions (LLMs) using Kaggle and Google Colab. Currently does 5 tests in Python Code and 5 tests in Math, hopefully can grow with translation, general question/answering and basic knowledge, speed (token/sec), and much more! 



3. Table of contents:

	1) LLMbenchmark Kaggle Notebook: LLM eval/benchmark code for Yi (6B), Vicuna (7B), Mistral (7B), and Gemma (2B), returns the benchmark table for each. More information and details in the first cell of the notebook itself. 
	2) Google Colab Notebook:
	3) YourOwnLLMbenchmark Kaggle Notebook: Copy the cells and change XXX with your own LLM name that you want to test in this evaluation. More information and details in the first cell of the notebook itself. 



4) How to install/run the project:

Kaggle = offers 30 hours of GPU 100 + 20 hours of TPU for FREE when an account has been created and verified. Simply create and account and verify it! 

Google Colab = free T4, if Pro then limited but extensive amount of GPU and TPU. Simply login to a google account!

Information and explanations on how to run each notebook will be in the top cell of each of them. 



5) How to use the project:

Currently, the LLMbenchmark Kaggle has 4 LLM eval/benchmark code: Yi (6B), Vicuna (7B), Mistral (7B), and Gemma (2B). The Google Colab version possesses 2 LLM eval/benchmark code: Yi (9B) and Gemma (7B). Again, information and explanations on how to run each notebook and evals will be in the notebook’s first cell. 

You can copy and recreate and eval for any LLM by copying the cells of a model and simply changing the model-specific parts with your model’s name. Using the YourOwnLLMbenchmark Kaggle Notebook will provide a format and by following the instructions in that notebook you may be able to recreate the eval but for the LLM of you choice. 



6) Credits and contact 
@ Celeste Deudon 



7) License: Apache 2.0 (view LICENSE.py)
